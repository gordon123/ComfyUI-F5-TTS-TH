import re, json, torch
from .F5TTS_Advance import F5TTS_Advance

class FairyTaleNarratorSwitcher:
    @classmethod
    def INPUT_TYPES(cls):
        model_choices = [
            "model_250000.pt", "model_250000_FP16.pt",
            "model_475000.pt", "model_475000_FP16.pt",
            "model_500000.pt", "model_500000_FP16.pt",
            "model_600000.pt", "model_600000_FP16.pt"
        ]
        required = {
            "text": ("STRING", {"multiline": True}),
            "sample_audio_narator": ("AUDIO",),
            "sample_text_narator": ("STRING", {"default": ""}),
            "model_name": (model_choices, {"default": "model_500000.pt"}),
            "seed": ("INT", {"default": -1, "min": -1}),
        }
        optional = {}
        # 5 à¸•à¸±à¸§à¸¥à¸°à¸„à¸£à¸£à¸­à¸‡
        for i in range(1,6):
            optional[f"char_name_{i}"]   = ("STRING", {"default": f"Character{i}"})
            optional[f"sample_audio_{i}"] = ("AUDIO",)
            optional[f"sample_text_{i}"]  = ("STRING", {"default": ""})
        # à¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œ TTS
        optional.update({
            "remove_silence":      ("BOOL",  {"default": True}),
            "speed":               ("FLOAT", {"default": 1.0,  "min": 0.1, "max": 5.0,  "step": 0.1}),
            "cross_fade_duration": ("FLOAT", {"default": 0.15, "min": 0.0, "max": 1.0,  "step": 0.01}),
            "nfe_step":            ("INT",   {"default": 32,   "min": 1,   "max": 128}),
            "cfg_strength":        ("FLOAT", {"default": 2.0,  "min": 0.0, "max": 10.0, "step": 0.1}),
            "sway_sampling_coef":  ("FLOAT", {"default": -1.0, "min": -5.0,"max": 5.0,  "step": 0.1}),
            "fix_duration":        ("FLOAT", {"default": 0.0,  "min": 0.0, "max": 30.0, "step": 0.1}),
            "max_chars":           ("INT",   {"default": 250,  "min": 1,   "max": 1000})
        })
        return {"required": required, "optional": optional}

    RETURN_TYPES = ("AUDIO", "STRING")
    RETURN_NAMES = ("audio", "text")
    FUNCTION = "run"
    CATEGORY = "ðŸ‡¹ðŸ‡­ Thai TTS"

    def run(self, text, sample_audio_narator, sample_text_narator,
            model_name="model_500000.pt", seed=-1, *args, **kwargs):

        # helper: à¸”à¸¶à¸‡ [à¸Šà¸·à¹ˆà¸­] à¹à¸¥à¸° à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ à¸«à¸¥à¸±à¸‡ ] à¹„à¸”à¹‰à¸—à¸¸à¸à¸à¸£à¸“à¸µ
        def parse_line(line):
            if line.startswith('['):
                end = line.find(']')
                if end != -1:
                    spk = line[1:end]
                    utt = line[end+1:].strip().strip('â€œâ€" ')
                    return spk, utt
            return None, line

        # à¸ªà¸£à¹‰à¸²à¸‡ map à¸‚à¸­à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡
        refs = {"narator": (sample_audio_narator, sample_text_narator)}
        for i in range(1,6):
            name = kwargs.get(f"char_name_{i}", "").strip()
            aud  = kwargs.get(f"sample_audio_{i}")
            txt  = kwargs.get(f"sample_text_{i}", "")
            if name and aud is not None:
                refs[name] = (aud, txt)

        # à¹à¸¢à¸à¸ªà¸„à¸£à¸´à¸›à¸•à¹Œà¹€à¸›à¹‡à¸™à¸šà¸£à¸£à¸—à¸±à¸”à¹à¸¥à¹‰à¸§à¸ˆà¸±à¸š segments
        segments = []
        for raw in text.splitlines():
            line = raw.strip()
            if not line: continue
            spk, utt = parse_line(line)
            if spk in refs:
                segments.append((spk, utt))
            else:
                # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ tag à¸«à¸£à¸·à¸­ tag à¹„à¸¡à¹ˆà¸•à¸£à¸‡à¹ƒà¸™ refs â†’ narator
                segments.append(("narator", utt if spk is None else utt))

        # à¹€à¸à¹‡à¸šà¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œ TTS
        f5_params = {k: kwargs[k] for k in [
            "remove_silence", "speed", "cross_fade_duration",
            "nfe_step", "cfg_strength", "sway_sampling_coef",
            "fix_duration", "max_chars"
        ] if k in kwargs}
        # à¸•à¹‰à¸­à¸‡à¹€à¸•à¸´à¸¡ model_name à¹à¸¥à¸° seed à¹€à¸‚à¹‰à¸²à¹„à¸›à¸”à¹‰à¸§à¸¢
        f5_params["model_name"] = model_name
        f5_params["seed"]       = seed

        # à¸ªà¸±à¸‡à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸—à¸µà¸¥à¸° segment
        tts = F5TTS_Advance()
        audio_chunks = []
        sr_out = None
        out_meta = []
        for spk, utt in segments:
            ref_aud, ref_txt = refs.get(spk, refs["narator"])
            audio_dict, _ = tts.synthesize(ref_aud, ref_txt, utt, **f5_params)
            wav  = audio_dict["waveform"]
            sr   = audio_dict["sample_rate"]
            sr_out = sr_out or sr
            if torch_imported and wav.dim()==1: wav = wav.unsqueeze(0)
            audio_chunks.append(wav)
            out_meta.append({"speaker": spk, "text": utt})

        # à¸•à¹ˆà¸­à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        if torch_imported and len(audio_chunks)>1:
            full = torch.cat(audio_chunks, dim=1)
        else:
            full = audio_chunks[0]

        return ({"waveform": full, "sample_rate": sr_out},
                json.dumps(out_meta, ensure_ascii=False))
